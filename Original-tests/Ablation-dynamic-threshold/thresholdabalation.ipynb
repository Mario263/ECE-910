{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import shutil\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from scenedetect import detect, ThresholdDetector, ContentDetector\n",
    "from multiprocessing import Pool, set_start_method\n",
    "import tempfile\n",
    "\n",
    "# ‚úÖ Set temp directory to avoid FileNotFoundError\n",
    "os.environ['TMPDIR'] = tempfile.gettempdir()\n",
    "\n",
    "# ‚úÖ Define Paths\n",
    "VIDEO_PATH = \"/Users/mario/Desktop/Desktop/UofA/4.Winter-2025/ece-910/Wild-Bunch-output/TheWildBunch(1969).mp4\"\n",
    "FRAMES_DIR = \"/Users/mario/Desktop/Desktop/UofA/4.Winter-2025/ece-910/Wild-Bunch-output/Frames-extraction\"\n",
    "SEQUENCES_DIR = \"/Users/mario/Desktop/Desktop/UofA/4.Winter-2025/ece-910/Wild-Bunch-output/histogram-output-sequences\"\n",
    "JSON_OUTPUT_DIR = \"/Users/mario/Desktop/Desktop/UofA/4.Winter-2025/ece-910/Wild-Bunch-output/json-output\"\n",
    "\n",
    "# ‚úÖ Ensure Directories Exist\n",
    "os.makedirs(FRAMES_DIR, exist_ok=True)\n",
    "os.makedirs(SEQUENCES_DIR, exist_ok=True)\n",
    "os.makedirs(JSON_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ‚úÖ Set FPS\n",
    "FPS = 60\n",
    "print(f\"üé• FPS of the movie: {FPS}\")\n",
    "\n",
    "\n",
    "# ‚úÖ Step 1: Extract Frames with Resizing and Progress Bar\n",
    "def extract_frames(video_path, output_dir, resize_width=224, resize_height=224):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_count = 0\n",
    "\n",
    "    print(\"\\nüéûÔ∏è Extracting and resizing frames...\")\n",
    "    for _ in tqdm(range(total_frames), desc=\"Extracting Frames\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        resized_frame = cv2.resize(frame, (resize_width, resize_height))\n",
    "        frame_path = os.path.join(output_dir, f\"frame_{frame_count:06d}.jpg\")\n",
    "        cv2.imwrite(frame_path, resized_frame)\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"‚úÖ Extracted and resized {frame_count} frames from the video.\")\n",
    "\n",
    "\n",
    "# ‚úÖ Step 2: Detect Hard and Gradual Cut Boundaries\n",
    "def detect_shot_boundaries(video_path):\n",
    "    hard_cut_scenes = detect(video_path, ThresholdDetector(threshold=30.0, min_scene_len=15))\n",
    "    hard_boundaries = [(scene[0].get_frames(), scene[1].get_frames()) for scene in hard_cut_scenes]\n",
    "    print(f\"‚úÖ Detected {len(hard_boundaries)} hard-cut sequences.\")\n",
    "\n",
    "    gradual_cut_scenes = detect(video_path, ContentDetector(threshold=30.0, min_scene_len=15))\n",
    "    gradual_boundaries = [(scene[0].get_frames(), scene[1].get_frames()) for scene in gradual_cut_scenes]\n",
    "    print(f\"‚úÖ Detected {len(gradual_boundaries)} gradual-cut sequences.\")\n",
    "\n",
    "    return hard_boundaries, gradual_boundaries\n",
    "\n",
    "\n",
    "# ‚úÖ Step 3: Organize Frames into Sequences and Save Metadata\n",
    "def organize_frames_by_sequence(frame_dir, boundaries, cut_type):\n",
    "    frames = sorted([f for f in os.listdir(frame_dir) if f.endswith(\".jpg\")])\n",
    "    sequences_metadata = {\"sequences\": []}\n",
    "\n",
    "    for i, (start, end) in enumerate(boundaries, 1):\n",
    "        seq_dir = os.path.join(SEQUENCES_DIR, f\"{cut_type}_sequence_{i}\")\n",
    "        os.makedirs(seq_dir, exist_ok=True)\n",
    "\n",
    "        for frame in frames:\n",
    "            frame_num = int(frame.split(\".\")[0].replace(\"frame_\", \"\"))\n",
    "            if start <= frame_num <= end:\n",
    "                shutil.copy(os.path.join(frame_dir, frame), os.path.join(seq_dir, frame))\n",
    "\n",
    "        sequences_metadata[\"sequences\"].append({\n",
    "            \"sequence_id\": i,\n",
    "            \"start_frame\": start,\n",
    "            \"end_frame\": end,\n",
    "            \"start_time\": start / FPS,\n",
    "            \"end_time\": end / FPS,\n",
    "            \"duration\": (end - start) / FPS\n",
    "        })\n",
    "\n",
    "    metadata_json_path = os.path.join(JSON_OUTPUT_DIR, f\"{cut_type}_metadata.json\")\n",
    "    with open(metadata_json_path, \"w\") as json_file:\n",
    "        json.dump(sequences_metadata, json_file, indent=4)\n",
    "\n",
    "    print(f\"‚úÖ {cut_type.capitalize()} sequence metadata saved at: {metadata_json_path}\")\n",
    "    return sequences_metadata\n",
    "\n",
    "\n",
    "# ‚úÖ Step 4: Compute Motion using Optical Flow\n",
    "def compute_motion(image1, image2):\n",
    "    if image1 is None or image2 is None:\n",
    "        return 0\n",
    "\n",
    "    gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY) if len(image1.shape) == 3 else image1\n",
    "    gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY) if len(image2.shape) == 3 else image2\n",
    "\n",
    "    flow = cv2.calcOpticalFlowFarneback(gray1, gray2, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    return np.mean(np.abs(flow))\n",
    "\n",
    "\n",
    "# ‚úÖ Function to Extract Image Embeddings using SigLIP\n",
    "def get_image_embedding(image_path, device, processor, model):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = model.get_image_features(**inputs)\n",
    "    return embedding.squeeze()\n",
    "\n",
    "\n",
    "# ‚úÖ Step 5: Extract Keyshots\n",
    "def extract_keyshots(sequence_dir, hist_threshold, siglip_threshold, device, processor, model):\n",
    "    frames = sorted([f for f in os.listdir(sequence_dir) if f.endswith(\".jpg\")])\n",
    "    keyshots = []\n",
    "    prev_hist = None\n",
    "    prev_embedding = None\n",
    "    prev_frame = None\n",
    "\n",
    "    for frame in frames:\n",
    "        frame_path = os.path.join(sequence_dir, frame)\n",
    "        image = cv2.imread(frame_path)\n",
    "        if image is None:\n",
    "            continue\n",
    "\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        hist = cv2.calcHist([gray_image], [0], None, [256], [0, 256])\n",
    "        hist = cv2.normalize(hist, hist).flatten()\n",
    "\n",
    "        if prev_hist is None:\n",
    "            keyshots.append(frame_path)\n",
    "            prev_hist = hist\n",
    "            prev_embedding = get_image_embedding(frame_path, device, processor, model)\n",
    "            prev_frame = gray_image\n",
    "            continue\n",
    "\n",
    "        hist_diff = cv2.compareHist(prev_hist, hist, cv2.HISTCMP_BHATTACHARYYA)\n",
    "        motion_score = compute_motion(prev_frame, gray_image)\n",
    "\n",
    "        if hist_diff > hist_threshold:\n",
    "            curr_embedding = get_image_embedding(frame_path, device, processor, model)\n",
    "            similarity = torch.nn.functional.cosine_similarity(prev_embedding, curr_embedding, dim=0)\n",
    "\n",
    "            if similarity < siglip_threshold or motion_score > 2.0:\n",
    "                keyshots.append(frame_path)\n",
    "                prev_embedding = curr_embedding\n",
    "                prev_frame = gray_image\n",
    "\n",
    "        prev_hist = hist\n",
    "\n",
    "    return keyshots\n",
    "\n",
    "\n",
    "# ‚úÖ Step 6: Process Threshold Combination\n",
    "def process_threshold_combination(args):\n",
    "    hist_thresh, siglip_thresh = args\n",
    "\n",
    "    # Initialize Model and Processor inside the Process\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    print(f\"\\nüöÄ Running with Hist_Threshold={hist_thresh} | SigLIP_Threshold={siglip_thresh}\")\n",
    "\n",
    "    for seq in os.listdir(SEQUENCES_DIR):\n",
    "        seq_path = os.path.join(SEQUENCES_DIR, seq)\n",
    "        if not os.path.isdir(seq_path):\n",
    "            continue\n",
    "\n",
    "        keyshots = extract_keyshots(seq_path, hist_thresh, siglip_thresh, device, processor, model)\n",
    "\n",
    "    json_path = os.path.join(JSON_OUTPUT_DIR, f\"optimized_results_hist-{hist_thresh}_siglip-{siglip_thresh}.json\")\n",
    "    with open(json_path, \"w\") as json_file:\n",
    "        json.dump({\"hist_thresh\": hist_thresh, \"siglip_thresh\": siglip_thresh, \"keyshots\": keyshots}, json_file, indent=4)\n",
    "\n",
    "    print(f\"‚úÖ JSON saved at: {json_path}\")\n",
    "\n",
    "\n",
    "# ‚úÖ Step 7: Run Experiments in Parallel using Multiprocessing\n",
    "def run_experiments():\n",
    "    extract_frames(VIDEO_PATH, FRAMES_DIR)\n",
    "    hard_boundaries, gradual_boundaries = detect_shot_boundaries(VIDEO_PATH)\n",
    "\n",
    "    organize_frames_by_sequence(FRAMES_DIR, hard_boundaries, \"hard_cuts\")\n",
    "    organize_frames_by_sequence(FRAMES_DIR, gradual_boundaries, \"gradual_cuts\")\n",
    "\n",
    "    hist_thresholds = [0.2, 0.3, 0.4, 0.5]\n",
    "    siglip_thresholds = [0.65, 0.70, 0.75, 0.8, 0.85, 0.9]\n",
    "    threshold_combinations = [(h, s) for h in hist_thresholds for s in siglip_thresholds]\n",
    "\n",
    "    # Start multiprocessing with set_start_method\n",
    "    with Pool() as pool:\n",
    "        pool.map(process_threshold_combination, threshold_combinations)\n",
    "\n",
    "    print(\"‚úÖ All experiments completed.\")\n",
    "\n",
    "\n",
    "# ‚úÖ Main Entry Point\n",
    "if __name__ == \"__main__\":\n",
    "    # Explicitly set the start method to 'spawn'\n",
    "    try:\n",
    "        set_start_method(\"spawn\")\n",
    "    except RuntimeError:\n",
    "        pass  # start method already set\n",
    "\n",
    "    run_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
